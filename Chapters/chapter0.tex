\newpage
\chapterstar{Introduction Générale}

Le machine learning connaît une croissance exponentielle depuis les années 2010, portée par l’augmentation de la puissance de calcul, la disponibilité massive de données et les avancées algorithmiques. Dans ce contexte, les algorithmes de classification et de régression constituent les piliers fondamentaux de l’apprentissage supervisé. Ils sont aujourd’hui déployés dans des domaines critiques tels que la médecine prédictive, la détection de fraude, la reconnaissance d’images ou encore l’analyse de sentiment.

Parmi ces applications, la détection de fraude bancaire occupe une place particulière en raison de ses enjeux économiques majeurs et de ses contraintes techniques spécifiques. Le problème se caractérise par un déséquilibre extrême des classes, une nécessité de prise de décision en temps réel, des coûts métier fortement asymétriques entre les erreurs, et des exigences réglementaires strictes en matière d’explicabilité des décisions. Dans ce cadre, la question centrale n’est pas seulement de maximiser la performance prédictive, mais de trouver un compromis réaliste entre efficacité, interprétabilité et viabilité opérationnelle.

Ce mémoire s’inscrit dans cette problématique et vise à proposer une analyse comparative rigoureuse de trois algorithmes majeurs du machine learning supervisé appliqués à la détection de fraude par carte de crédit : les Support Vector Machines (SVM), les Random Forests et XGBoost. Bien qu’ils poursuivent le même objectif de prédiction, ces modèles reposent sur des fondements théoriques radicalement différents et présentent des comportements distincts face aux contraintes du monde bancaire réel.

L’objectif de ce travail est triple. Premièrement, il s’agit de présenter les bases théoriques de chaque algorithme de manière précise, en mettant l’accent sur les propriétés réellement pertinentes pour la détection de fraude. Deuxièmement, ce mémoire vise à fournir un cadre d’aide à la décision permettant d’orienter le choix de l’algorithme en fonction des caractéristiques du problème, en intégrant à la fois des métriques académiques classiques et des indicateurs métier concrets. Troisièmement, ce travail cherche à challenger les approches existantes en proposant des configurations algorithmiques alternatives capables d’améliorer les performances observées dans la littérature récente.

Le document est structuré en trois chapitres complémentaires. Le premier chapitre est consacré aux fondements théoriques des trois algorithmes étudiés. Il présente le principe de maximisation de la marge des SVM et le rôle du noyau dans la gestion de la non-linéarité, l’approche ensembliste des Random Forests basée sur le bagging et la réduction de la variance, ainsi que le principe du gradient boosting séquentiel implémenté par XGBoost, accompagné de ses mécanismes avancés de régularisation. Une attention particulière est portée aux stratégies de gestion du déséquilibre des classes, élément central de la détection de fraude.

Le deuxième chapitre propose une analyse comparative des trois approches selon des critères directement liés aux contraintes opérationnelles du domaine bancaire. Sont notamment étudiés la complexité algorithmique, le temps d’entraînement, la latence de prédiction, la gestion native ou artificielle du déséquilibre, l’interprétabilité des modèles, leur robustesse face à l’évolution des comportements frauduleux, ainsi que leur consommation mémoire en contexte de déploiement. Cette analyse vise à dépasser une comparaison purement académique pour adopter une perspective orientée production.


Le troisième chapitre constitue le cœur expérimental de ce mémoire. Il présente une application pratique sur un jeu de données réel de transactions bancaires anonymisées. Plusieurs configurations algorithmiques sont implémentées et comparées, incluant des variantes de SVM avec pondération des classes, de Random Forest avec différentes techniques de rééchantillonnage, ainsi que de XGBoost exploitant ses mécanismes natifs de gestion du déséquilibre. L’évaluation ne se limite pas aux métriques classiques de classification, mais intègre un modèle de coût réaliste afin de mesurer l’impact financier réel des décisions de chaque algorithme.

Un accent particulier est mis sur l’optimisation du seuil de décision. Contrairement à l’approche standard utilisant un seuil fixe, ce travail adopte une stratégie orientée métier visant à maximiser le bénéfice net, en tenant compte du fait qu’en détection de fraude, les faux négatifs sont généralement bien plus coûteux que les faux positifs. L’impact du feature engineering sur la performance des modèles est également étudié afin d’évaluer la sensibilité de chaque algorithme à la qualité des représentations d’entrée.

La conclusion synthétise les résultats théoriques et empiriques obtenus afin de formuler des recommandations claires et pragmatiques. Elle répond notamment aux questions suivantes : dans quels contextes privilégier une Random Forest ? Quand l’investissement dans le réglage fin de XGBoost est-il justifié ? Les SVM conservent-ils une pertinence dans les systèmes modernes de détection de fraude bancaire ? À travers cette analyse, ce mémoire ambitionne de contribuer à une meilleure compréhension des compromis réels entre performance, coût et explicabilité dans les systèmes de détection de fraude en production.
